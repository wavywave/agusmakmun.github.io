---
layout: post
title:  "TIL 210419"
description: "머신러닝 복습"
author: SeungRok OH
categories: [TIL]
---


# 2021.04.19 TIL

- 머신러닝 복습.



# 판별모형

## SVM



데이터를 분류할때 데이터 사이 판별경계선은 어떻게 그어야 할까. 새로운 테스트 데이터를 잘 예측하기 위해 데이터와 데이터 사이의 마진값을 높이는 것이 테스트 성능을 높이는데 도움이 될 것이다.

마진을 최대한으로 하는 판별 경계선을 긋는다고 할때 최전방의 data를 '서포트 벡터'라고 하며 서포트 벡터의 길이가 1이 되도록 정한다.

![1](https://user-images.githubusercontent.com/77723966/115417187-61a27280-a233-11eb-9143-467a844625ab.PNG)

마진 값을 크게 만든다는 얘기는 곧 ∥w∥ 을 최소로 만든다는 것과 동일하며 각 거리를 곱하여 2로 나눈 1/2wTw의 값을 최소로 하는 최적화 문제를 푸는 것과 동일해진다.

![2](https://user-images.githubusercontent.com/77723966/115417212-67985380-a233-11eb-8447-9ae127b4bdbc.PNG)

부등식 문제이므로 N개의 부드익 조건을 만족하는 문제는 곧 라그랑주 승수를 사용한 최적화 문제로 연결된다.

KKT조건 (①dh/dx = 0 ② dh/dλ * λ = 0 ③ λ >=0)을 만족시키는 문제로 부등식 제한조건의 문제는 결국 제한조건이 없거나(λ = 0) 등식 제한조건을 푸는 문제와 동일함을 배웠다. 이 경우 서포트 벡터를 제외한 데이터는 λ=0으로 고려대상에서 제외할 수 있다. 



### scikit - learn에서의 SVM

![3](https://user-images.githubusercontent.com/77723966/115417237-6cf59e00-a233-11eb-8592-ab859e02c51a.PNG)

슬랙변수 = 판별경계선(직선)으로 나누어지지 않는, 선형분리가 불가능할 경우 슬랙변수를 사용하여 데이터의 개별오차를 허용한다.

- y(wTx - w0) <= -1 + ξ
- ξ >= 0 

인수인 c값을 크게 주게 되면 마진이 적어지더라도 슬랙변수를 줄이려 한다.  각각의 특성이 존재하는데, C가 클때는 마진이 적어 데이터를 보다 잘 분류할 수 있지만 서포트벡터에 의존하여 데이터가 조금만 달라지더라도 모양이 크게 변하는 반면 C가 작으면 서포트 벡터, 계산량이 많아지지만 마진이 넓어지고 판별선이 쉽게 변하지는 않는다.(성능분산이 낮다는 얘기)



### Kernel SVM(커널 서포트 벡터 머신)

data가 복잡해질 수록 직선으로 분류하기 힘든 상황이 벌어진다.(ex. Xor문제) 특히 비선형적인 데이터가 그러한데 이럴 경우 판별모형이 복잡해질 수 있다.



비선형 데이터를 제대로 분류할 수 있는 판별함수를 생각하는 것은 쉬운일이 아니다. 따라서 이럴 경우 판별모형은 그대로 선형으로 남겨둔채 데이터feature를 비선형함수를 이용하여 변형시켜 새로운 feature로 만들어 적용시킨다. 

X -> Φ(X) -> 선형판별모형(f = wTΦ), 이 경우 x에 대해서는 비선형이지만 w에 대해서는 선형함수가 되어 판별모형을 어렵게 생각하지 않아도 된다.



문제는 Φ(X)라는 새로운 비선형함수를 생각해야 하는 어려움이 생기게 되는데 이는 커널 트릭으로 해결할 수 있다.

최종 수식인 목적함수, 판별함수에서 Φ(X)의 쓰임을 확인해보면 독립적으로 사용되는 것이 아닌 내적(스칼라 변환)되어 사용되는 것을 확인할 수 있다.

![4](https://user-images.githubusercontent.com/77723966/115417272-72eb7f00-a233-11eb-94ad-6b698ec66578.PNG)


많이 사용되는 커널은 다항커널, 가우시안 커널, 시그모이드 커널등이 있다.



# 모형 최적화

머신러닝 모형 완성 후 최적화 과정을 통해 예측 성능 향상

- Validation_curve : 단일 하이퍼 파라미터 최적화
- GridSearchCV : 그리드를 사용해 복수 하이퍼 파라미터 최적화
- ParameterGrid : 복수 파라미터 최적화 그리드(수동)



ex) GridSerachCV

![5](https://user-images.githubusercontent.com/77723966/115417292-7848c980-a233-11eb-8d7f-68c99bc32961.PNG)

병렬처리(GridSearchCV를 더 효율적으로)

![6](https://user-images.githubusercontent.com/77723966/115417317-7d0d7d80-a233-11eb-8c75-e90f919d5f2e.PNG)

n_jobs = n_thread 코드를 통해 cpu개수에 비례하여 독립적으로 실행, 빠르게 계산한다.

n_jobs란 인수는 의사결정나무, 랜덤포레스트에도 존재한다. 각 모델은 독립적으로 실행하는 것이기에 존재.

