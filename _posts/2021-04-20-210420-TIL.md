---
layout: post
title:  "TIL 210420"
description: "머신러닝 복습"
author: SeungRok OH
categories: [TIL]
---

# 2021.04.20 TIL

- 머신러닝 복습.



# 모형 최적화

### 대규모 데이터 

대규모 데이터의 경우 1) 종류가 많거나(too many, feature) 2) 개수가 많거나 (too much)

1) 종류가 많을때, Feature selection 통하여 일부만 사용한다.

- 분산에 의한 선택(y와 상관없이 고정되어 있거나 변동 폭이 매우 낮은 애들을 제거한다.)
- 단일 변수 선택(카이제곱 검정 통계값을 이용)
- 특성중요도 계산하여 중요 변수만 사용(Feature importance)



분산을 기준으로 사용하는 방법

![1](https://user-images.githubusercontent.com/77723966/115417525-a75f3b00-a233-11eb-9ecb-6dc83666fe40.PNG)



2) 개수가 많을때

사용할 수 있는 모형이 한정되어 있음. data 경우 골라서 사용할 수 없기에 selection 을 사용하지는 않음.

SGD / 나이브 베이즈 주로 사용 (SVM은 사용불가)

랜덤포레스트 같은 경우 모델 생성시 'warm_start =True' 인수 사용하여 점진적 적용 가능.



### 비대칭 데이터

희귀병 분류와 같이 y변수, 클래스가 비대칭적으로 존재할 경우 제대로된 분류가 이루어지지 않을 수도 있다.

단순히 정확도가 관건이라면 크게 어려움이 없지만 목적이 탐지(detect)일 경우(recall rate를 높이는 것) 얘기가 달라진다.

두 가지 방법을 사용하게 되는데 1) 판별모형에서 threshold를 낮추기 2) data를 강제로 늘리거나 줄이는 방법이 있다.

2) Data를 강제로 늘리거나 줄이는 방법 (pip install -U imbalanced-learn 패키지를 설치한다.)

![2](https://user-images.githubusercontent.com/77723966/115417556-ad551c00-a233-11eb-988c-47646d335807.PNG)

RandomUnderSampler : 무작위로 데이터 제거 (주로 다수 데이터가 제거될 수밖에 없음.)

TomekLinks : 가장 근거리에 있는 데이터가 다른 클래스일 경우 두 데이터를 연결하여 TomekLink라 부른다. 주로 판별 경계선 또는 아웃라이어 주변에 많이 생긴다.

CondensedNearestNeighbour : TomekLinks와 반대로 경계선에 있는 데이터를 남기는 방법. 소수데이터의 경우 남기고 다수데이터 중 가장 근거리에 있는 데이터가 같은 클래스일 경우 포함시키지 않음.

ENN : CondensedNearestNeighbour와 비슷하지만 가장 근거리가 아닌 근거리에 있는 K개의 데이터를 분류하여 소수 데이터가 많이 잡힐 경우 해당 데이터를 삭제.

OneSidedSelection : TomekLinksd와 CondensedNearestNeighbour 합친 방법





RandomOverSampler : 가중치 늘리기 단순 데이터 복붙(위치도 동일)

ADASYN : 소수와 소수 데이터 사이(근거리 위주) 새로운 소수 데이터 생성시키는 방법.

SMOTE : 단순 소수데이터만 생성하는 것이 아닌 분류 모형을 적용하여 다수 데이터에 가깝게 위치해있다면 다수 데이터도 생성하는 방법



# 비지도학습

## 군집화 (Clustering)



K-means 군집화 : 군집이 똑같은 크기에 원형이라는 가정을 하여 분류. 타원이라 비대칭 데이터 잘 분류하지 못한다는 단점이 있음.

성능 평가 기준 : 조정랜드지수, 조정상호정보량 (클래스를 알고 있을 경우에 사용) / 실루엣 계수 (클래스를 모를 경우)



일치행렬 : 랜드지수를 구하기 위한 행렬

답안지 행렬과 군집화 결과 행렬을 비교하여 동일하면 1로 그렇지 않다면 0으로 표시한다.



랜드지수 : 일치 행렬 위쪽 비대각 성분에서 1의 개수 / nC2(전체 데이터 n개에서 2개를 한쌍으로, 전체 쌍의 개수)

조정랜드지수 : 랜드지수의 경우 무작위로 군집화를 하여도 좋은 성능이 나올 가능성이 높음. 이를 어느정도 완하하여 표준화와 비슷한 처리를 한게 조정랜드지수

![4](https://user-images.githubusercontent.com/77723966/115417616-b8a84780-a233-11eb-8018-103f25651995.PNG)

실루엣계수 : 라벨링이 되어 있지 않는 데이터를 다룰때 군집의 성능 판단 기준

![5](https://user-images.githubusercontent.com/77723966/115417632-bc3bce80-a233-11eb-95f6-f4aac4c0f7c1.PNG)


군집(클러스터링)개수가 몇개인지 계수를 보고 trial & error 후 판단해야 한다. 

동심원 데이터의 경우 실루엣계수가 정답이어도 낮을 수 있음. (labeling를 하거나 도메인 지식이용)



### K-means(K-평균) 군집화 

![6](https://user-images.githubusercontent.com/77723966/115417651-bfcf5580-a233-11eb-93d1-86961eb68448.PNG)

데이터(xi)와 군집의 중심위치(uk)간의 거리를 최소화/ 군집의 개수 미리 설정해야한다.

1. 임의의 중심위치 uk를 선택(보통 표본 중에서 선택)

2. 모든 데이터에서 각각의 중심위치 uk까지 거리 계산

3. 각 데이터에서 가장 가까운 중심위치를 선택하여 군집을 정한다.

4. 중심위치 uk 다시 계산

5. 2~4 반복 

![7](https://user-images.githubusercontent.com/77723966/115417675-c4940980-a233-11eb-8f60-9229ccfc85e2.PNG)



K ++ means 군집화 : 초기값이 서로 붙어 있지 않도록 설정해주는 군집화 방법.

중심 하나를 임의로 정하고 다른 하나는 거리에 비례해 가중치확률을 주어서 선택하게끔 함. 세개 이상이라면 모든 샘플간 거리를 구하여 가장 가까운 것을 새로운 중심으로 한번 더 가중치 확률을 부여
