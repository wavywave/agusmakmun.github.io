---
layout: post
title:  "TIL 210401"
description: "기초 통계 지식 복습"
author: SeungRok OH
categories: [TIL]
---

# 2021.04.01 TIL

- 기초통계



# 엔트로피

확률이 다름정도를 숫자로 나타내는 것. 특정값에 확률이 몰려 있으면 엔트로피가 적음

![1](https://user-images.githubusercontent.com/77723966/113296606-a625a700-9334-11eb-8aa9-101790ea07c6.PNG)

ex)  엔트로피 그래프

p0 = np.linspace(0.001, 1-0.001, 1000)

p1 = 1 - p0

H =  - p0 * np.log2(p0) - p1 * np.log2(p1)

G = 2 * (P0 * (1 - P0) + P1 * (1 - P1))

plt.plot(p1, H, '-', label='엔트로피')

plt.plot(p1, G, '--', label='지니불순도')

plt.legend()

plt.xlabel('P(Y=1)')

plt.show()



대부분 데이터로 주어져 있기 때문에 데이터를 통해 확률분포를 추정하여 엔트로피를 구해야 한다.



### 지니 불순도

엔트로피 경우 log식이 포함되어 계산에 있어 시간이 오래 소요. log 대신 근사값인 1-p(Yk)를 사용.

추후 '의사결정나무'에서 엔트로피와 함께 사용 됨.



### 가변길이 인코딩

글자의 확률분포를 통해 가장 많은 수의 글자를 인코딩할때 적은 수로 변환

가변길이 인코딩 후 나온 수의 개수는 원 글자 개수 * 엔트로피 계산 값과 같다.

엔트로피 값이 높이 나올 수록 글자가 고르게 분포(정보가 많다.) 엔트로피 값니 낮을 수록 글자가 편향되어 분포(유의미한 정보가 적다.)



### 조건부 엔트로피

#### 결합엔트로피

1차원 -> 다차원 

#### 조건부엔트로피

어떤 확률변수 X가 다른 확률변수 Y를 예측하는데 도움이 되는지 측정하는 방법 중 하나.

![2](https://user-images.githubusercontent.com/77723966/113296631-ade54b80-9334-11eb-84c8-93e6946f4971.PNG)


조건이 정해져있지 않은 경우 '가중 평균'을 한다. 

즉, x를 조건으로 사용할 경우 p(x)의 확률을 가중하여 엔트로피의 평균을 낸다.

x가 다른 조건인 경우의 y 엔트로피가 낮아지는지 확인하여 낮아진다면 x가 y를 분류하는데 있어 도움이 됨.

![3](https://user-images.githubusercontent.com/77723966/113296642-b3429600-9334-11eb-9479-80ba11838ac1.PNG)


y값 0.4 / 0.6으로 의미없어보이지만 x가 결정될때 y역시 결정되는, 분류에 있어 도움이 될 수 있다.

상관계수가 낮더라도 조건부 엔트로피를 통해 도움을 받을 수 있는 경우도 있다. (선형적인 모형만 있는건 아니니까)



- 붓꽃 데이터의 경우

![4](https://user-images.githubusercontent.com/77723966/113296664-be95c180-9334-11eb-84aa-90f63890c06b.PNG)


def colc_cond_entropies(col, threshold):

​	df['X1'] = df[col] > threshold

​	pivot_table= \

​		df.groupby(['X1', 'species']).size().unstack().fillna(0)

​	v = pivot_table_values

​	pyx0 = v[0, :] / np.sum(v[0, :])

​	pyx1 = v[1, :] / np.sum(v[1, :])

​	Hyx0 = sp.stats.entropy(pyx0, base=2)

​	Hyx1 = sp.stats.entropy(pyx1, base=2)



#### 교차 엔트로피

H[p,q],  결합엔트로피와 모양이 비슷해보이나 확률변수가 들어가는 결합엔트로피와 달리 확률분포함수(pdf)가 들어간다. 

![5](https://user-images.githubusercontent.com/77723966/113296683-c3f30c00-9334-11eb-96df-d70aad0e2d14.PNG)


p, p가 아닌 p(yk), q(yk) 가 들어간다.  p는 정답일 확률분포, q는 예측값의 확률분포.

분류문제의 성능을 측정할때 사용.



#### 쿨백-라이블러 발산

두 확률분포의 p(y), q(y)의 분포 모양이 얼마나 다른지 숫자로 계산한 값.

거리와 유사도 개념은 아니지만 비슷함.

![6](https://user-images.githubusercontent.com/77723966/113296704-c8b7c000-9334-11eb-95d9-fe27809a890c.PNG)


#### 상호 정보량

상관계수를 대체할 수 있는 지수. (비선형적인 상관관계를 가시화하는데 도움을 주는)

쿨백-라이블러 발산을 사용하여 X,Y 두 변수의 결합함수와 각각의 함수의 곱을 계산

두 확률분포가 같으면 0이되고 두 확률분포는 서로 독립이 된다.
