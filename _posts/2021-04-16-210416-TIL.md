---
layout: post
title:  "TIL 210416"
description: "머신러닝 복습"
author: SeungRok OH
categories: [TIL]
---


# 2021.04.16 TIL

- 머신러닝 복습.



# 머신러닝 모형

### 의사결정나무



여러 독립 변수 중 하나의 독립 변수를 선택하고 기준값을 정하여 (분류 규칙) 데이터를 분류한다.

기준값 보다 크거나 작은 데이터집단 두 가지로 나누어 자식노드라 명명한다.

이 과정을 반복하여 하위의 자식 노드를 더 만드는데 한 종류의 클래스의 데이터만 존재한다면 분류를 멈춘다. 



한 번에 기준 하나를 선택하여 분류하기 때문에 설명-해석하기 쉽다.



기준값을 정하는 기준은? = 정보획득량이 가장 커지는 방향으로 

![1](https://user-images.githubusercontent.com/77723966/115032870-c5136400-9f04-11eb-80a0-331dcccd158f.PNG)

각 변수 데이터 간 사이 값을 기준으로 분류를 실행하여 정보획득량이 가장 커지는 방향의 기준값을 설정한다.

조건부 엔트로피 , X1, X2 두가지가 있을때 각 변수의 확률을 가중치로하여 가중치를 곱한뒤 서로 더해서 구해준다.

모든 데이터 사이 값을 기준값으로 분류를 하는것이 아니라 프로그램이 적당한 데이터 사이 값을 기준값으로 설정한다. (모든 데이터 사이값을 전부 사용하지 않는다는 의미)

순수하게 나뉘어지거나, 설정한 Depth에 도달하거나, 분류 후 데이터가 너무 적다면 분류를 멈추게 된다.



tree1 = DecisioniTreeClassifier(criterion='entropy', max_depth=1, random_state=1).fit(X, y)

이런식으로 구현하고 criterion 경우 디폴트값은 '지니불순도'(엔트로피와 유사 but 로그 사용하지 않아 계산량이 적다.)이며 엔트로피로도 설정가능하다. 자식노드를 몇개까지 만들 수 있는지 정하는 max_depth도 있다. 모든 데이터 값을 사용하지 않으니 random_state 인수 역시 존재한다.



의사결정나무의 문제점 : Greedy한 의사결정. 

선택된 특징이 지금 단계에서는 최적일 수 있지만 다음, 다다음단계의 관점에서 보았을때 효율적인 선택이 아닐 경우가 발생한다. 



# 앙상블

복수의 예측 모형 결합 (분류문제에서만)



단일 모형을 사용할때 보다 성능분산이 감소하고 과적합을 방지한다. (train - test를 나누어 test 성능을 확인했을때 항상 같게 나오지 않으니 범주가 생기는데 앙상블을 사용하면 그 범주가 줄어들게 된다.)

개별모형이 성능이 그렇게 좋지 않을 경우 모이면 성능이 향상된다. 단 성능이 아예 안 좋은 경우라면 똑같이 안좋아진다. 향상되진 않는다.

크게 '취합'방법론과 '부스팅'방법론으로 나뉜다.

취합은 다수결 모형, 배깅, 랜덤포레스트가 있으며 사용할 모형의 집합이 이미 결정되어 있다. 각 모형은 같은 문제를 풀어 답을 내린다.

부스팅은 에이다 부스트, 그레디언트 부스트가 있으며 사용할 모형을 점진적으로 늘려간다는 특징이 있다. 취합과 달리 같은 문제를 푸는 것이 아니라 '점진적' 즉, 두번째 모델은 첫번째 모델이 잘 풀지 못했던 문제를 풀게 된다.



## 취합

### 다수결 방법

Hard voting : 단순 투표, 개별 모형의 결과 기준 (ex. 각 모형당 1표)

Soft voting : 가중치 투표, 개별 모형의 조건부 확률 합 기준(ex. 가중치에 따라 표 분배)

![2](https://user-images.githubusercontent.com/77723966/115032897-cb094500-9f04-11eb-928f-8b9b05d20a42.PNG)

![3](https://user-images.githubusercontent.com/77723966/115032919-ce9ccc00-9f04-11eb-9bf3-14c4b5548c9a.PNG)

soft와 hard, Vote 종류에 따라 결과값이 다르게 나올 수 있다.



### 배깅

같은 모형을 쓰되 다른 결과를 출력하는 다수의 모형을 만들어 사용한다. 부트스트래핑과 유사하게 데이터 내에 데이터를 랜덤하게 선택하여 다수결 모형을 적용한다. 

아웃라이어가 포함될 확률이 적어지므로 성능분산이 작아진다. 

![4](https://user-images.githubusercontent.com/77723966/115032939-d3618000-9f04-11eb-9747-8359e323368f.PNG)



### 랜덤포레스트

기본모델이 의사결정나무를 사용할때 사용 가능. 기본적으로 subspace 방법을 사용.

하나의 '노드'마다 다른 한 종류의 데이터를 주어 분류를 시킨다. (X1 -X20 까지의 변수들이 있고 기본 의사결정나무에서 X1-X10까지만 사용한다고 했을때 자식 노드 역시 X1-X10만 사용하게 되지만 랜덤포레스트 경우 자식 노드는 X1-X10 밖에 있는 X11-X20을 사용하게 된다.)

greedy 한 선택을 방지할 수 있음.

Extremely Randomized Trees 모형도 존재. 이것은 변수 역시 고르는 것이 아닌 랜덤으로 택하여 분류하는 것. (물론 threshold-기준은 선택한다.)

정보획득량을 기준으로 feature importance를 확인한다.



## 부스팅

하나의 모형에서 시작하여 모형 집합에 포함할 개별 모형을 하나씩 추가한다. 

모형의 집합은 위원회라고하여 C라 표시하며 위원회에 들어가는 개별 모형을 약 분류기라 하여 K로 표시한다.

전 단계에서 제대로 처리하지 못하는 부분을 처리하기 위해 새로운 K를 선택한다. 최종 결정도 다수결이 아닌 멤버에 따라 가중치를 준다.



### 에이다 부스트

데이터를 잘 예측했느냐 못했느냐에 따라 점수를 부여하여 맞았다면 0점 틀렸다면 1점을 부여하여 틀리게 에측한 데이터의 가중치를 합한 값을 손실함수 L로 사용한다. 일종의 벌점인데 이 손실함수를 최소화하는 모형이 다음 개별모형으로 선택된다. 

![5](https://user-images.githubusercontent.com/77723966/115032961-d8263400-9f04-11eb-8cf5-1862e29f7543.PNG)

y와 Km은 1또는 -1 

두 개의 문제가 남는데 하나는 해당 데이터에 관해 가중치를(w) 어떻게 설정할것이냐의 문제고 다른 하나는 개별 모형으로 뽑힌 멤버한테 가중치를(αm) 어떻게 부여할것이냐의 문제가 그것이다. 

![6](https://user-images.githubusercontent.com/77723966/115032983-dceae800-9f04-11eb-8da0-c4ebf3e0f72e.PNG)

εm의 경우 총 벌점 분의 해당 모형의 벌점이므로 낮을수록 좋다.  αm은 εm에 영향을 받는데 비유적으로 성적이 좋으면 투표권(가중치)을 많이 부여한다.

w의 경우 처음엔 모든 데이터들이 같은 값을 가지도록 한다.  Cm-1 (새로운 K가 들어오기전) 못맞추면 약 2점을 크게 얻고 맞추면 1/2로 적게 얻는다.  지금까지 모은 모형들로 분류했을때 못맞춘 데이터는 점수가 커지고 맞춘 데이터는 점수가 적어진다.



멤버수가 늘어나면 자동적으로 과적합이 증가하게 되어 정규화가 필요해진다.

에이다 부스트는 '학습속도 조정'을 통해 정규화를 꾀한다. Cm = Cm-1 + μαmKm . μ 인수를 1보다 적게주어 새로운 멤버의 가중치를 낮추는 방법이다. 물론 성능이 안 좋아지기는 한다.



### 그레디언트 부스트 (Decision Tree 만 사용)

변분법을 사용한 모형으로 함수의 최적화에서 Xm = Xm-1 - αm* df/dx 와 같은 공식을 이용했는데 여기서 αm* df/dx = stepsize , 스텝사이즈 조절을 통해 함수를 최소화하는 x를 찾아가는 방법을 배웠었다.

이 방법과 마찬가지로 그레디언트 부스트의 경우 손실 범함수를 최소화하는 개별 분류함수 Km을 찾는다.

새로운 x위치 찾아가기 ≒ 새로운 위원회 함수 찾아가기

![7](https://user-images.githubusercontent.com/77723966/115033000-e2483280-9f04-11eb-85cb-dd563ec73d51.PNG)

즉 그레디언트 부스트 경우 잔차라는 회귀분석 함수를 이용하여 놓친 부분을 메꾸는 것이다.

Ex)



![8](https://user-images.githubusercontent.com/77723966/115033014-e5dbb980-9f04-11eb-995f-ddcbfc48ac41.PNG)


**모델로는 그레디언트 부스트 자체(알고리즘)는 사용하지 않고 XGBoost 와 LightGBM ''패키지''들을 사용한다.**



